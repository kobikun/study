{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUs and LSTMs -- for machine translation\n",
    "* http://cs224d.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-01.png](./slide/CS224d-Lecture8-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-02.png](./slide/CS224d-Lecture8-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-03.png](./slide/CS224d-Lecture8-03.png)\n",
    "![CS224d-Lecture8-04.png](./slide/CS224d-Lecture8-04.png)\n",
    "![CS224d-Lecture8-05.png](./slide/CS224d-Lecture8-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "![CS224d-Lecture8-06.png](./slide/CS224d-Lecture8-06.png)\n",
    "![CS224d-Lecture8-07.png](./slide/CS224d-Lecture8-07.png)\n",
    "\n",
    "* 추가적인 정보\n",
    " * 지난 스터디 자료 : https://drive.google.com/file/d/0B-ZSzubU1u02aVFBeFpDQ3cteWc/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment \n",
    "\n",
    "* https://en.wikipedia.org/wiki/IBM_alignment_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-08.png](./slide/CS224d-Lecture8-08.png)\n",
    "![CS224d-Lecture8-09.png](./slide/CS224d-Lecture8-09.png)\n",
    "![CS224d-Lecture8-10.png](./slide/CS224d-Lecture8-10.png)\n",
    "![CS224d-Lecture8-11.png](./slide/CS224d-Lecture8-11.png)\n",
    "![CS224d-Lecture8-12.png](./slide/CS224d-Lecture8-12.png)\n",
    "* -------------------------------------------------------------------------------\n",
    "* syntax-based model ![tor-geht-schnell-auf-parallel](./images/tor-geht-schnell-auf-parallel.png)\n",
    " * http://www.statmt.org/moses/?n=Moses.SyntaxTutorial\n",
    "* -------------------------------------------------------------------------------\n",
    "![CS224d-Lecture8-13.png](./slide/CS224d-Lecture8-13.png)\n",
    "![CS224d-Lecture8-14.png](./slide/CS224d-Lecture8-14.png)\n",
    "![CS224d-Lecture8-15.png](./slide/CS224d-Lecture8-15.png)\n",
    "> Reference<br/>\n",
    "> Kevin Knight Tutorial : http://www.cs.pomona.edu/~dkauchak/mt-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMT\n",
    "![nmt](./images/smt_nmt-624x351.png)\n",
    "> Graphical illustration of Neural MT, SMT+Reranking-by-NN and SMT-NN. From [Bahadanau et al., 2015] slides at ICLR 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning & Machine Translation \n",
    "* ![figure1_encoder-decoder1-300x126](./images/figure1_encoder-decoder1-300x126.png)\n",
    "> Encoder-Decoder for Machine Translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-16.png](./slide/CS224d-Lecture8-16.png)\n",
    "![CS224d-Lecture8-17.png](./slide/CS224d-Lecture8-17.png)\n",
    "> https://en.wikipedia.org/wiki/Softmax_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT & RNN Model Extenstions\n",
    "![CS224d-Lecture8-18.png](./slide/CS224d-Lecture8-18.png)\n",
    "![CS224d-Lecture8-19.png](./slide/CS224d-Lecture8-19.png)\n",
    "![CS224d-Lecture8-20.png](./slide/CS224d-Lecture8-20.png)\n",
    "> The very first neural machine translation system.\n",
    "\n",
    "![CS224d-Lecture8-21.png](./slide/CS224d-Lecture8-21.png)\n",
    "* reversing ; for solving long term problem.\n",
    "![CS224d-Lecture8-22.png](./slide/CS224d-Lecture8-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data.\n",
    "\n",
    "#  Encoder\n",
    "## Step 1: A word to a one-hot vector.\n",
    "![Figure-3_one-hot.png](./images/Figure-3_one-hot.png)\n",
    "* $w_i$ projects with matrix E : vocabulary size * (100-500) \n",
    "* $s_i = Ew_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :  A one-hot vector to a continuous-space representation.\n",
    "![Figure4_continuous-space](./images/Figure4_continuous-space.png)\n",
    "* RNN : has two key capabilities which are sequence summarization and probabilistic modeling of sequences.\n",
    "* summarizing a sequence \n",
    "* $h_i = \\phi_\\theta(h_{i-1}, s_i)$\n",
    " * $h_0$ : all-zero vector. \n",
    "\n",
    "> $h_T$ : 입력 소스 문장 전체애 대한 summary를 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sequence summarization by a recurrent neural network.\n",
    "![Figure5_summarization](./images/Figure5_summarization.png)\n",
    "\n",
    "\n",
    "## mathematics\n",
    "![Figure_3-624x208](./images/Figure_3-624x208.png)\n",
    "* $h_t = tanh(Wx_t + Uh_{t-1}+b)$ \n",
    " * W : input weight matrix \n",
    " * U : recurrent weight matrix\n",
    " * b : bias.\n",
    " \n",
    "* activation functions\n",
    "![activation_functions](./images/activations_functions.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2-D Visualization of Sentence Representations from [Sutskever et al., 2014]. Similar sentences are close together in summary-vector space.\n",
    "![Figure6_summary_vector_space](./images/Figure6_summary_vector_space.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decoder\n",
    "##  Step 1: Computing the internal hidden state of the decoder.\n",
    "![Figure7_internal-hidden-state](./images/Figure7_internal-hidden-state.png)\n",
    "> $z_i=\\phi_{\\theta`}(h_t,u_{i-1},z_{i-1})$ : chain rule?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Next word probability.\n",
    "![Figure8_scoring](./images/Figure8_scoring.png)\n",
    "> $e(k) = w_k^T z_i+b_k$ : $w_k$ and $b_k$ are target word vector and a bias\n",
    "\n",
    "### Normalization\n",
    "* softmax \n",
    "> $p(w_i=k|w_1,w_2,...,w_{i-1},h_T) = \\frac{ exp(e(k)) }{ \\sum_j exp(e(j))  }  $\n",
    "* can use to select a word by sampling the distribution.\n",
    "* repeating until we select the end-of-sentence word(<eos>).\n",
    "\n",
    "## REF.\n",
    "* http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/\n",
    "* troble and overcome on nmt\n",
    " * http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs : gated recurrent units \n",
    "\n",
    "![CS224d-Lecture8-23.png](./slide/CS224d-Lecture8-23.png)\n",
    "* RNN Tutorial Part 3 - BPTT와 Vanishing Gradient 문제 : http://aikorea.org/blog/rnn-tutorial-3/\n",
    "> 다행히도, 이 문제를 어느 정도 해결할 수 있는 몇 가지 방법이 있다. W 행렬을 적당히 좋은 값으로 잘 초기화 해준다면 vanishing gradient의 영향을 줄일 수 있고, regularization을 잘 정해줘도 비슷한 효과를 볼 수 있다. 더 보편적으로 사용되는 방법은 tanh나 sigmoid activation 함수 말고 ReLU를 사용하는 것이다. ReLU는 미분값의 최대치가 1로 정해져있지 않기 때문에 gradient 값이 없어져버리는 일이 크게 줄어든다. 이보다 더 인기있는 해결책은 Long Short-Term Memory (LSTM)이나 Gated Recurrent Unit (GRU) 구조를 사용하는 방법이다. LSTM은 1997년에 처음 제안되었고, 현재 자연어처리 분야에서 가장 널리 사용되는 모델 중 하나이다. GRU는 2014년에 처음 나왔고, LSTM을 간략화한 버전이다. 두 RNN의 변형 구조 모두 vanishing gradient 문제 해결을 위해 디자인되었고, 효과적으로 긴 시퀀스를 처리할 수 있다는 것이 보여졌다. 이 구조들에 대해서는 다음 파트에서 다룰 것이다.\n",
    "\n",
    "### math\n",
    "\n",
    "* $r_t =  \\sigma (W_r x_t + U_r h_{t-1}+b_r)$\n",
    "* $u_t =  \\sigma (W_u x_t + r_t \\bigodot  (U_u h_{t-1}) + b_u)$\n",
    "* $h_t = u_t \\bigodot  h_{t-1} + (1-u_t ) \\bigodot  tanh(Wx_t + r_t \\bigodot (U_u h_{t-1}+b)$\n",
    "\n",
    "![CS224d-Lecture8-24.png](./slide/CS224d-Lecture8-24.png)\n",
    "![CS224d-Lecture8-25.png](./slide/CS224d-Lecture8-25.png)\n",
    "![CS224d-Lecture8-26.png](./slide/CS224d-Lecture8-26.png)\n",
    "![CS224d-Lecture8-27.png](./slide/CS224d-Lecture8-27.png)\n",
    "![CS224d-Lecture8-28.png](./slide/CS224d-Lecture8-28.png)\n",
    "![CS224d-Lecture8-29.png](./slide/CS224d-Lecture8-29.png)\n",
    "![CS224d-Lecture8-30.png](./slide/CS224d-Lecture8-30.png)\n",
    "* LSTM이 대세.\n",
    "* http://arxiv.org/pdf/1409.3215v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs\n",
    "![./images/lecture_note_gru.png](./images/lecture_note_gru.png)\n",
    "\n",
    "## LSTM\n",
    "![./images/lecture_note_lstm.png](./images/lecture_note_lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CS224d-Lecture8-31.png](./slide/CS224d-Lecture8-31.png)\n",
    "![CS224d-Lecture8-32.png](./slide/CS224d-Lecture8-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trouble & Solutions?\n",
    "\n",
    "* ![Figure1_BLEUscore_vs_sentencelength-624x459](./images/Figure1_BLEUscore_vs_sentencelength-624x459.png)\n",
    "> Dramatic drop of performance w.r.t. the length of sentence with a small encoder-decoder model.\n",
    "\n",
    "## Soft Attention Mechanism for Neural Machine Translation\n",
    "* ![Figure2_biRNN](./images/Figure2_biRNN.png)\n",
    "> Bidirectional recurrent neural networks for encoding a source sentence.<br/>\n",
    "> A B C D -> X Y Z<br/>\n",
    "> D C B A -> X Y Z \n",
    "\n",
    "![Figure3_attention_1-624x352](./images/Figure3_attention_1-624x352.png)\n",
    "> Attention Mechanism takes into consideration what has been translated and one of the source words.\n",
    "\n",
    "![Figure4_attention_2-624x352](./images/Figure4_attention_2-624x352.png)\n",
    "> Attention Mechanism returns a single scalar corresponding to a relevance score of the j-th source word.\n",
    "\n",
    "## wow\n",
    "![Figure6_sample_translations1-624x282](./images/Figure6_sample_translations1-624x282.png)\n",
    "> Sample translations made by the neural machine translation model with the soft-attention mechanism. Edge thicknesses represent the attention weights found by the attention model.\n",
    "\n",
    "![Screen-Shot-2015-12-30-at-1.23.48-PM](./images/Screen-Shot-2015-12-30-at-1.23.48-PM.png)\n",
    "> http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n",
    "\n",
    "![Figure7_RNNsearch-501-624x356](./images/Figure7_RNNsearch-501-624x356.png)\n",
    "> RNNsearch-50 is a neural machine translation model with the attention mechanism trained on all the sentence pairs of length at most 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "\n",
    "* Introduction to Neural Machine Translation with GPUs\n",
    " * http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/\n",
    " * http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/\n",
    " * http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/\n",
    "\n",
    "* ChainerとRNNと機械翻訳\n",
    " * http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e\n",
    " \n",
    "* http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
